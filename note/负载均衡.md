[toc]

# 一、负载均衡

网络分层

![](https://cdn.jsdelivr.net/gh/eaok/img/net/net_layered.png)

> 基于网络7层协议，负载均衡可分为二层，三层，四层，七层；



二层负载均衡
负载均衡服务器对外依然提供一个VIP（虚IP），集群中不同的机器采用相同IP地址，但是机器的MAC地址不一样。当负载均衡服务器接受到请求之后，通过改写报文的目标MAC地址的方式将请求转发到目标机器实现负载均衡；



三层负载均衡
负载均衡服务器对外依然提供一个VIP（虚IP），但是集群中不同的机器采用不同的IP地址。当负载均衡服务器接受到请求之后，根据不同的负载均衡算法，通过IP将请求转发至不同的真实服务器器；



四层负载均衡

工作在传输层
基于IP+端口的负载均衡，工作在第四层的产品有LVS/F5/Haproxy等
优点：只做转发，网卡是瓶颈，内存/CPU基本不消耗
缺点：应对DDos能力差



七层负载均衡
OSI最高层，可以对于应用层的请求进行解析并实行一系列策略，工作在第七层的产品有：Nginx/Haproxy
优点：根据语义进行定制化负载，流量量智能化
缺点：处理理能力差，需要消耗更多资源



# 二、负载均衡技术

## LVS

LVS集群采用IP负载均衡技术和基于内容请求分发技术。调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序。



LVS术语

```
DS：Director Server。指的是前端负载均衡器节点
RS：Real Server。后端真实的工作服务器器
VIP：向外部直接面向⽤用户请求，作为⽤用户请求的目标的IP地址
DIP：Director Server IP，主要⽤用于和内部主机通讯的IP地址
RIP：Real Server IP，后端服务器的IP地址
CIP：Client IP，访问客户端的IP地址
```



### LVS-NAT模式

LVS NAT原理理：用户请求LVS到达director，director将请求的报文的目的IP改为RIP，同时将报文的目标端口也改为realserver的相应端口，最后将报文发送到realserver上，realserver将数据返回给director，director再把数据发送给用户LVS NAT特性：
NAT模式修改的是目的ip，直接走的是switch不需要修改mac地址，所以VIP和RIP不需要在同一个网段内
NAT的包的进出都需要经过LVS，所以LVS可能会成为一个系统的瓶颈问题

![](https://cdn.jsdelivr.net/gh/eaok/img/net/lvs-nat.png)



### LVS FullNAT模式

LVS FULLNAT特性 ：
●  FULLNAT模式也不需要DIP和RIP在同一网段
●  FULLNAT和NAT相比会保证RS的回包一定可到达LVS
●  FULLNAT需要更新源IP，所以性能正常比NAT模式下降10%

![](https://cdn.jsdelivr.net/gh/eaok/img/net/lvs-fullnat.png)



### LVS-DR模式

LVS DR原理理：用户请求LVS到达director，director将请求的报文的目的MAC地址改为后端的realserver的MAC地址，目的IP为VIP(不变)，源IP为client IP地址(不变)，然后director将报文发送到realserver，realserver检测到目的地址为自己本地的VIP，如果在同一网段，将请求直接返回给用户，如果用户跟realserver不在同一个网段，则需要通过网关返回给用户。

LVS DR特性：
前端路由将目标地址为VIP报文统统发给Director Server，RS跟Director Server必须有一个网卡在同一个物理网络中，所有的请求报文经由Director Server，但响应报文必须不能进过Director Server，所有的real server机器上都有VIP地址

![](https://cdn.jsdelivr.net/gh/eaok/img/net/lsv-dr.png)



### LVS TUN模式

LVS TUN原理理：用户请求LVS到达director，director通过IP-TUN加密技术将请求报文的包封装到一个新的IP包里面，目的IP为VIP(不不变)，然后director将报文发送到realserver，realserver基于IP-TUN解密，然后解析出来包的目的为VIP，检测网卡是否绑定了VIP，绑定了就处理这个包，如果在同一个网段，将请求直接返回给用户，否则通过网关返回给用户；如果没有绑定VIP就直接丢掉这个包

LVS TUN特性：
TUNNEL必须在所有的realserver上绑定VIP，realserver直接把包发给client

![](https://cdn.jsdelivr.net/gh/eaok/img/net/lvs-tun.png)



四种模式比较

1. 是否需要VIP和realserver在同一网段
   DR模式因为只修改包的MAC地址，需要通过ARP广播找到realserver，所以VIP和realserver必须在同一个网段，也就是说DR模式需要先确认这个IP是否只能挂在这个LVS下面；其他模式因为都会修改目的地址为realserver的IP地址，所以不需要在同一个网段内
2. 是否需要在realserver上绑定VIP
   realserver在收到包之后会判断目的地址是否是自己的IP，DR模式的目的地址没有修改，还是VIP，所以需要在realserver上绑定VIP，IP TUN模式值是对包重新包装了了⼀一层，realserver解析后的包的IP仍然是VIP，所以也需要在realserver上绑定VIP
3. 四种模式的性能比较
   DR模式、IP TUN模式都是在包进⼊入的时候经过LVS，在包返回的时候直接返回给client；所以二者的性能比NAT高，但TUN模式更加复杂，所以性能不如DR，FULLNAT模式不仅更换目的IP还更换了源IP，所以性能比NAT下降10%
   性能比较：DR>TUN>NAT>FULLNAT



## Nginx反向代理

### UpStream

Upstream是反向代理理服务器器池；

轮询(weight=1)

```nginx
upstream bakend {
    server 192.168.1.100;
    server 192.168.1.110;
}
```

权重

```nginx
upstream bakend {
    server 192.168.1.100 weight=1;
    server 192.168.1.110 weight=2;
}
```

ip_hash

```nginx
upstream bakend {
    ip_hash;
    server 192.168.1.100:8080;
    server 192.168.1.110:8080;
}
```

fair(第三方插件)

```nginx
upstream bakend {
    server 192.168.1.100:8080;
    server 192.168.1.110:8080;
    fair;
}
```

url_hash

```nginx
upstream bakend {
    server 192.168.1.100:8080;
    server 192.168.1.110:8080;
    hash $request_uri;
    hash_method crc32;
}
```



例子：

```nginx
upstream tel_img_steam {
    #ip_hash;
    server 192.168.11.68:20201;
    
    #down表示单前的server暂时不参与负载
    server 192.168.11.69:20201 weight=100 down;
    server 192.168.11.70:20201 weight=100;
    
    #backup 备⽤用服务器, 其它所有的非backup机器down或者忙的时候，请求backup机器
    server 192.168.11.71:20201 weight=100 backup;
    
    #max_fails 允许请求失败的次数默认为1
    #fail_timeout max_fails次失败后，暂停的时间
    server 192.168.11.72:20201 weight=100 max_fails=3 fail_timeout=30s;
}
```





### Proxy_Pass

Proxy_pass是进行反向代理的规则

```
proxy_method：修改⽤用户的method请求
proxy_http_version：修改⽤用户的http协议版本
proxy_set_header：修改⽤用户header头部，如客户端真实IP信息，也是配置较多的选项
proxy_set_body：修改⽤用户包体信息
proxy_send_timeout：默认60S
proxy_connect_timeout：默认60S，Nginx与后端服务器连接超时间
proxy_next_upstream 如果满足条件就进行重试
```



RoundRobin策略

```nginx
http {		#upstream模块包在http模块下
	upstream myserver {			#定义upstream名字
		server 192.168.1.100;	#指定后端服务器地址
		server 192.168.1.110;	#指定后端服务器地址
		server 192.168.1.120;	#指定后端服务器地址
	}
	server {
		listen 80;
		server name www.myserver.com;
		location / {
			proxy_pass http://myserver;	#引用upstream
		}
	}
}
```

加权策略

```nginx
http {
    upsteam myserver {
        server 192.168.1.100 weight=3;	#指定后端服务器地址，权重为3
        server 192.168.1.110;
    }
    server {
        listen 80;
        server name www.myserver.com;
        location / {
            proxy_pass http://myserver;
        }
    }
}
```

IP-Hash策略

```nginx
upstream myserver {
    ip_hash;				#采用IP-Hash算法
    server 192.168.1.100;
    server 192.168.1.110;
    server 192.168.1.120;
}
```

高可用策略

```nginx
http {
    upstream myserver {
        server 192.168.1.100;
        server 192.168.1.110 backup;
    }
    server {
        listen 80;
        server name www.myserver.com;
        location / {
            proxy_pass http://myserver;
        }
    }
}
```



案例1：

```nginx
#在nginx.conf中定义一组upstream
upstream pre-cloud_Backend {
    server pre-cloud.website.com:8080;
    ip_hash;
    check interval=5000 rise=1 fall=3 timeout=30000;
    check_http_expect_alive http_2xx http_3xx;	#tengine的健康检查模块
}

#conf.d/下定义一个文件
server {
    listen 80;
    server_name cloud.website.com;
    limit_conn perserver 10000;
    location / {
        proxy_net_upstream error timeout http_503 http504 http502;	#如果请求的当前节点出发了这里指定的条件就将请求转发到下一个机器
        proxy_connect_timeout 500s;
        proxy_read_timeout 500s;
        proxy_send_timeout 500s;
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_pass http://pre-cloud_Backend;
    }
}
```





## cdn技术

> CDN技术Content Delivery Network：内容分发网络，对于不同地域进行请求的Cache，就近选择伺服
> 缺点：对于更新响应策略需要考虑
> 静态资源受用

### Squid

Squid是一个缓存internet数据的一个软件，它接收用户的下载申请，并自动处理所下载的数据。

http://www.squid-cache.org/



### Nginx proxy_cache

利用Nginx的proxy_cache搭建：Nginx中的proxy_cache组件可以使得从远端请求的源数据保存在本地，从而实现一个CDN网络的搭建



## keepalived技术

![](https://cdn.jsdelivr.net/gh/eaok/img/net/keepalived.png)



## dns技术

通过配置不同区域的DNS设置进行入口机器的负载均衡
1.  AWS Route 53
2.  NSONE
3.  Dyn
4.  dnspod

上述的域名服务商有相关服务



## 硬件负载

f5是一家公司，提供硬件负载服务

![](https://cdn.jsdelivr.net/gh/eaok/img/net/f5.png)



# 三、常见负载均衡技术组合

* 独立7层

  ![](https://cdn.jsdelivr.net/gh/eaok/img/net/7layer.png)

* 7层+Backup

  ![](https://cdn.jsdelivr.net/gh/eaok/img/net/7layer_keepalived.png)

* 4层+7层

  ![](https://cdn.jsdelivr.net/gh/eaok/img/net/4layer_7layer.png)

*  DNS+4层+Backup+7层+Backup

  ![](https://cdn.jsdelivr.net/gh/eaok/img/net/dns_4layer_7layer.png)

